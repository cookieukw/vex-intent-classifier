  const translations = {
    en: {
      sidebar_title: "Classifier Analysis",
      nav_objective: "Objective & Scope",
      nav_architecture: "Hybrid Architecture",
      nav_training: "Training Phase",
      nav_prediction: "Prediction Phase",
      nav_optimizations: "Optimizations",
      nav_conclusion: "Conclusion",
      nav_manage_data: "Manage Data",
      nav_explorer: "Interactive Explorer",
      objective_title: "1. Objective and Scope of `IntentClassifier`",
      objective_p1:
        'The `IntentClassifier` acts as an <strong>NLU (Natural Language Understanding)</strong> engine. Its primary function is to receive a text input from the user (an "utterance") and classify it into a predefined set of categories known as "intents." This component is fundamental for dialogue systems, such as chatbots, as it translates human language into structured commands that a system can process.',
      objective_li1:
        '<strong>Input:</strong> A text string (e.g., <code>"what is the status of my order?"</code>).',
      objective_li2:
        "<strong>Processing:</strong> Lexical and semantic analysis of the text.",
      objective_li3:
        '<strong>Output:</strong> The most likely intent (e.g., <code>"check_order_status"</code>) and an associated response.',
      architecture_title: "2. Hybrid Classification Architecture",
      architecture_p1:
        "The classifier implements a <strong>two-stage hybrid approach</strong>, a strategy that aims to balance precision, recall, and performance. This pipeline architecture ensures that simple cases are resolved quickly, while more complex cases receive a deeper analysis.",
      flowchart_title: "Classification Flow",
      flowchart_input: "User Input",
      flowchart_step1_title: "Stage 1: String Similarity",
      flowchart_step1_desc:
        "Search for exact or misspelled matches via <strong>Levenshtein Distance</strong>.",
      flowchart_question: "Match Found?",
      flowchart_yes: "YES",
      flowchart_no: "NO",
      flowchart_return: "Return Intent",
      flowchart_step2_title: "Stage 2: Semantic Similarity",
      flowchart_step2_desc:
        "Meaning analysis with <strong>TF-IDF</strong> and <strong>Cosine Similarity</strong>.",
      training_title: "3. Analysis of the Training Process (`async train()`)",
      training_p1:
        "The `train()` method is responsible for preparing the classifier, transforming raw training data into usable mathematical models. This process is executed once before predictions can be made.",
      training_h3_1: "3.1. Corpus Collection and Preparation",
      training_p2:
        'Initially, data is loaded and processed. A "document" is created for each intent, aggregating all its training phrases. The `nlp.cleanAndTokenize` function performs crucial preprocessing tasks like tokenization, normalization, stopword removal, and stemming.',
      training_h3_2: "3.2. The Vector Space Model: TF-IDF",
      training_p3:
        "The heart of the semantic model is <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>, a statistical technique that assigns a weight to each word in a document, reflecting its importance.",
      training_idf_title: "a) IDF (Inverse Document Frequency)",
      training_idf_desc:
        'Measures the <strong>rarity</strong> of a word across the entire dataset. Rare words (e.g., "invoice") are more informative and receive a higher weight.',
      training_tf_title: "b) TF (Term Frequency)",
      training_tf_desc:
        "Measures the <strong>frequency</strong> of a word within a single document (intent). Words that appear more often are more relevant to that specific topic.",
      training_p4:
        'The final TF-IDF weight is the product of these two values, resulting in a <strong>vector</strong> that represents the semantic "fingerprint" of each intent.',
      prediction_title: "4. Analysis of the Prediction Process (`predict()`)",
      prediction_p1:
        "This method executes the two-stage pipeline to classify a new user message.",
      prediction_h3_1:
        "4.1. Stage 1: String Similarity Classification (Levenshtein)",
      prediction_p2:
        'The <strong>Levenshtein Distance</strong> is an algorithm that measures the "edit distance" between two strings. It calculates the minimum number of character insertions, deletions, or substitutions required to change one string into the other, making it ideal for catching typos. The implementation uses a dynamic threshold for greater flexibility.',
      prediction_h3_2:
        "4.2. Stage 2: Semantic Similarity Classification (Cosine)",
      prediction_p3:
        "If Stage 1 fails, the system resorts to <strong>Cosine Similarity</strong>. This metric measures the angle between the TF-IDF vectors of the user's message and each intent. A result close to 1 indicates high semantic similarity, while a result close to 0 indicates a low relationship. The result is only returned if it surpasses a predefined confidence threshold.",
      optimizations_title: "5. Architecture Analysis and Optimizations",
      optimizations_h3_1: "5.1. Strengths",
      optimizations_li1:
        "<strong>Hybrid Architecture:</strong> Robust combination of string-based and semantic methods.",
      optimizations_li2:
        "<strong>Client-Side Execution:</strong> Low latency and enhanced data privacy.",
      optimizations_li3:
        "<strong>Robustness:</strong> Correctly handles edge cases and unseen terms.",
      optimizations_h3_2: "5.2. Performance Optimization: Caching",
      optimizations_p1:
        "The main suggested optimization is <strong>caching</strong>. In the original implementation, training phrases are cleaned and tokenized on every `predict()` call. The solution is to preprocess these phrases once during the `train()` method and cache them. This moves the expensive computation to a one-time initialization phase, making subsequent predictions significantly faster and more scalable.",
      optimizations_code: "Example of optimization in the train() method",
      conclusion_title: "6. Conclusion",
      conclusion_p1:
        "The `IntentClassifier.ts` is a solid and successful implementation of an intent classifier for NLU. Its hybrid architecture results in an effective system. Applying optimizations, such as caching preprocessed data, can further enhance its performance for large-scale applications.",
      manage_data_title: "7. Manage Training Data",
      manage_data_p1:
        "Add new intents or more training phrases to existing ones to see how the classifier adapts. Changes are for this simulation only and will be lost on page reload.",
      manage_data_add_title: "Add New Intent",
      manage_data_name_label: "Intent Name",
      manage_data_name_placeholder: "e.g., product_info",
      manage_data_phrases_label: "Training Phrases (comma-separated)",
      manage_data_phrases_placeholder:
        "e.g., what is the price?, how much does it cost, tell me more about it",
      manage_data_response_label: "AI Response",
      manage_data_response_placeholder: "e.g., The price is $50.00.",
      manage_data_add_btn: "Add Intent",
      manage_data_add_phrase_btn: "Add Phrase",
      manage_data_phrases_title: "Training Phrases:",
      manage_data_response_title: "AI Response:",
      explorer_title: "üß™ Interactive Classifier Explorer",
      explorer_p1:
        "Enter a sentence below to simulate how the classifier would process your input. The simulation uses the training data above to demonstrate the two-stage logic in action.",
      explorer_label: "Your message:",
      explorer_placeholder: "Ex: hello, how are you?",
      explorer_button: "Classify",
      explorer_stage1_title:
        "Stage 1 Analysis: String Similarity (Levenshtein)",
      explorer_stage2_title: "Stage 2 Analysis: Semantic Similarity (Cosine)",
      explorer_final_title: "Final Result",
      sim_found_match: "Found a close match!",
      sim_closest_phrase: "Closest training phrase:",
      sim_lev_distance: "Levenshtein Distance:",
      sim_threshold: "Acceptance threshold:",
      sim_success: "Result:",
      sim_success_text: "Match found. Direct classification.",
      sim_no_match: "No string similarity match was found.",
      sim_failure_text: "Failure. Proceeding to Stage 2.",
      sim_comparing: "Comparing your message vector with each intent vector:",
      sim_intent_classified: "Classified Intent:",
      sim_confidence: "Confidence:",
      sim_ai_response: "AI Response:",
      sim_not_recognized: "Intent not recognized.",
      sim_confidence_low: "The highest confidence was",
      sim_below_threshold: "which is below the threshold of",
      sim_fallback_response: "Sorry, I didn't understand what you meant.",
    },
    pt: {
      sidebar_title: "An√°lise do Classificador",
      nav_objective: "Objetivo e Escopo",
      nav_architecture: "Arquitetura H√≠brida",
      nav_training: "Fase de Treinamento",
      nav_prediction: "Fase de Predi√ß√£o",
      nav_optimizations: "Otimiza√ß√µes",
      nav_conclusion: "Conclus√£o",
      nav_manage_data: "Gerenciar Dados",
      nav_explorer: "Explorador Interativo",
      objective_title: "1. Objetivo e Escopo da Classe `IntentClassifier`",
      objective_p1:
        'O <code>IntentClassifier</code> funciona como um motor de <strong>NLU (Natural Language Understanding)</strong>. Sua fun√ß√£o prim√°ria √© receber uma entrada de texto do usu√°rio (uma "utterance") e classific√°-la dentro de um conjunto pr√©-definido de categorias, conhecidas como "inten√ß√µes". Este componente √© fundamental para sistemas de di√°logo, como chatbots, pois traduz a linguagem humana em comandos estruturados que um sistema pode processar.',
      objective_li1:
        '<strong>Entrada:</strong> Uma string de texto (ex: <code>"qual o status do meu pedido?"</code>).',
      objective_li2:
        "<strong>Processamento:</strong> An√°lise l√©xica e sem√¢ntica do texto.",
      objective_li3:
        '<strong>Sa√≠da:</strong> A inten√ß√£o mais prov√°vel (ex: <code>"verificar_status_pedido"</code>) e uma resposta associada.',
      architecture_title: "2. Arquitetura H√≠brida de Classifica√ß√£o",
      architecture_p1:
        "O classificador implementa uma <strong>abordagem h√≠brida em duas etapas</strong>, uma estrat√©gia que visa equilibrar precis√£o, recall e performance. Esta arquitetura em pipeline garante que casos simples sejam resolvidos rapidamente, enquanto casos mais complexos recebem uma an√°lise mais profunda.",
      flowchart_title: "Fluxo de Classifica√ß√£o",
      flowchart_input: "Entrada do Usu√°rio",
      flowchart_step1_title: "Etapa 1: Similaridade de String",
      flowchart_step1_desc:
        "Busca por correspond√™ncia exata ou com erros de digita√ß√£o via <strong>Dist√¢ncia de Levenshtein</strong>.",
      flowchart_question: "Match Encontrado?",
      flowchart_yes: "SIM",
      flowchart_no: "N√ÉO",
      flowchart_return: "Retorna Inten√ß√£o",
      flowchart_step2_title: "Etapa 2: Similaridade Sem√¢ntica",
      flowchart_step2_desc:
        "An√°lise de significado com <strong>TF-IDF</strong> e <strong>Similaridade de Cosseno</strong>.",
      training_title: "3. An√°lise do Processo de Treinamento (`async train()`)",
      training_p1:
        "O m√©todo <code>train()</code> √© respons√°vel por preparar o classificador, transformando os dados brutos de treinamento em modelos matem√°ticos utiliz√°veis. Este processo √© executado uma √∫nica vez antes que as predi√ß√µes possam ser feitas.",
      training_h3_1: "3.1. Coleta e Prepara√ß√£o do Corpus",
      training_p2:
        'Inicialmente, os dados s√£o carregados e processados. Um "documento" √© criado para cada inten√ß√£o, agregando todas as suas frases de treinamento. A fun√ß√£o <code>nlp.cleanAndTokenize</code> executa tarefas de pr√©-processamento cruciais como tokeniza√ß√£o, normaliza√ß√£o, remo√ß√£o de stopwords e stemming.',
      training_h3_2: "3.2. O Modelo de Espa√ßo Vetorial: TF-IDF",
      training_p3:
        "O cora√ß√£o do modelo sem√¢ntico √© o <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>, uma t√©cnica estat√≠stica que atribui um peso a cada palavra em um documento, refletindo sua import√¢ncia.",
      training_idf_title: "a) IDF (Inverse Document Frequency)",
      training_idf_desc:
        'Mede a <strong>raridade</strong> de uma palavra em todo o conjunto de dados. Palavras raras (ex: "boleto") s√£o mais informativas e recebem um peso maior.',
      training_tf_title: "b) TF (Term Frequency)",
      training_tf_desc:
        "Mede a <strong>frequ√™ncia</strong> de uma palavra dentro de um √∫nico documento (inten√ß√£o). Palavras que aparecem mais vezes s√£o mais relevantes para aquele t√≥pico espec√≠fico.",
      training_p4:
        'O peso final TF-IDF √© a multiplica√ß√£o desses dois valores, resultando em um <strong>vetor</strong> que representa a "impress√£o digital" sem√¢ntica de cada inten√ß√£o.',
      prediction_title: "4. An√°lise do Processo de Predi√ß√£o (`predict()`)",
      prediction_p1:
        "Este m√©todo executa o pipeline de duas etapas para classificar uma nova mensagem do usu√°rio.",
      prediction_h3_1:
        "4.1. Etapa 1: Classifica√ß√£o por Similaridade de String (Levenshtein)",
      prediction_p2:
        'A <strong>Dist√¢ncia de Levenshtein</strong> √© um algoritmo que mede a "dist√¢ncia de edi√ß√£o" entre duas strings. Ele calcula o n√∫mero m√≠nimo de inser√ß√µes, exclus√µes ou substitui√ß√µes de caracteres para transformar uma string na outra, sendo ideal para capturar erros de digita√ß√£o. A implementa√ß√£o utiliza um limiar din√¢mico para maior flexibilidade.',
      prediction_h3_2:
        "4.2. Etapa 2: Classifica√ß√£o por Similaridade Sem√¢ntica (Cosseno)",
      prediction_p3:
        "Se a Etapa 1 falhar, o sistema recorre √† <strong>Similaridade de Cosseno</strong>. Esta m√©trica mede o √¢ngulo entre os vetores TF-IDF da mensagem do usu√°rio e de cada inten√ß√£o. Um resultado pr√≥ximo a 1 indica alta semelhan√ßa sem√¢ntica, enquanto um resultado pr√≥ximo a 0 indica baixa rela√ß√£o. O resultado s√≥ √© retornado se ultrapassar um limiar de confian√ßa pr√©-definido.",
      optimizations_title: "5. An√°lise de Arquitetura e Otimiza√ß√µes",
      optimizations_h3_1: "5.1. Pontos Fortes",
      optimizations_li1:
        "<strong>Arquitetura H√≠brida:</strong> Combina√ß√£o robusta de m√©todos baseados em string e sem√¢ntica.",
      optimizations_li2:
        "<strong>Execu√ß√£o Local (Client-Side):</strong> Baixa lat√™ncia e maior privacidade dos dados.",
      optimizations_li3:
        "<strong>Robustez:</strong> Lida corretamente com casos extremos e termos n√£o vistos.",
      optimizations_h3_2: "5.2. Otimiza√ß√£o de Performance: Caching",
      optimizations_p1:
        "A principal otimiza√ß√£o sugerida √© o **caching**. Na implementa√ß√£o original, as frases de treinamento s√£o limpas e tokenizadas a cada chamada de <code>predict()</code>. A solu√ß√£o √© pr√©-processar essas frases uma √∫nica vez durante o m√©todo <code>train()</code> e armazen√°-las em cache. Isso move a computa√ß√£o cara para uma fase de inicializa√ß√£o, tornando as predi√ß√µes subsequentes significativamente mais r√°pidas e escal√°veis.",
      optimizations_code: "Exemplo de otimiza√ß√£o no m√©todo train()",
      conclusion_title: "6. Conclus√£o",
      conclusion_p1:
        "O <code>IntentClassifier.ts</code> √© uma implementa√ß√£o s√≥lida e bem-sucedida de um classificador de inten√ß√µes para NLU. Sua arquitetura h√≠brida resulta em um sistema eficaz. A aplica√ß√£o de otimiza√ß√µes, como o caching de dados pr√©-processados, pode aprimorar ainda mais sua performance para aplica√ß√µes em larga escala.",
      manage_data_title: "7. Gerenciar Dados de Treino",
      manage_data_p1:
        "Adicione novas inten√ß√µes ou mais frases de exemplo √†s inten√ß√µes existentes para ver como o classificador se adapta. As altera√ß√µes s√£o apenas para esta simula√ß√£o e ser√£o perdidas ao recarregar a p√°gina.",
      manage_data_add_title: "Adicionar Nova Inten√ß√£o",
      manage_data_name_label: "Nome da Inten√ß√£o",
      manage_data_name_placeholder: "ex: informacoes_produto",
      manage_data_phrases_label: "Frases de Treino (separadas por v√≠rgula)",
      manage_data_phrases_placeholder:
        "ex: qual o pre√ßo?, quanto custa, me fale mais sobre",
      manage_data_response_label: "Resposta da IA",
      manage_data_response_placeholder: "ex: O pre√ßo √© R$ 50,00.",
      manage_data_add_btn: "Adicionar Inten√ß√£o",
      manage_data_add_phrase_btn: "Adicionar Frase",
      manage_data_phrases_title: "Frases de Treino:",
      manage_data_response_title: "Resposta da IA:",
      explorer_title: "üß™ Explorador Interativo do Classificador",
      explorer_p1:
        "Digite uma frase abaixo para simular como o classificador processaria sua entrada. A simula√ß√£o usa os dados de treino acima para demonstrar a l√≥gica de duas etapas em a√ß√£o.",
      explorer_label: "Sua mensagem:",
      explorer_placeholder: "Ex: ol√°, tudo bem?",
      explorer_button: "Classificar",
      explorer_stage1_title:
        "An√°lise da Etapa 1: Similaridade de String (Levenshtein)",
      explorer_stage2_title:
        "An√°lise da Etapa 2: Similaridade Sem√¢ntica (Cosseno)",
      explorer_final_title: "Resultado Final",
      sim_found_match: "Encontrada correspond√™ncia pr√≥xima!",
      sim_closest_phrase: "Frase de treino mais pr√≥xima:",
      sim_lev_distance: "Dist√¢ncia de Levenshtein:",
      sim_threshold: "Limiar de aceita√ß√£o:",
      sim_success: "Resultado:",
      sim_success_text: "Match encontrado. Classifica√ß√£o direta.",
      sim_no_match:
        "Nenhuma correspond√™ncia por similaridade de string foi encontrada.",
      sim_failure_text: "Falha. Prosseguindo para a Etapa 2.",
      sim_comparing:
        "Comparando o vetor da sua mensagem com o vetor de cada inten√ß√£o:",
      sim_intent_classified: "Inten√ß√£o Classificada:",
      sim_confidence: "Confian√ßa:",
      sim_ai_response: "Resposta da IA:",
      sim_not_recognized: "Inten√ß√£o n√£o reconhecida.",
      sim_confidence_low: "A maior confian√ßa foi de",
      sim_below_threshold: "o que est√° abaixo do limiar de",
      sim_fallback_response: "Desculpe, n√£o entendi o que voc√™ quis dizer.",
    },
  };